\documentclass[a4paper,11pt]{jsarticle}


% 数式
\usepackage{amsmath,amsfonts}
\usepackage{bm}
% 画像
\usepackage[dvipdfmx]{graphicx}
% 図の先頭につく，fig.1とか図１とか（キャプション）のカスタマイズ
\usepackage{caption}
\captionsetup[figure]{labelformat=simple, labelsep=space, textfont=normalfont, labelfont=bf}
\renewcommand{\figurename}{Fig.}


\begin{document}

\title{ロボメックの指針}
\author{Shibuya Atsushi 渋谷享史}
\date{\today}
\maketitle

要約：本研究は，従来の制御方法では未知の要因を含むため対処しきれない問題を，強化学習を用いて得た制御方法に部分的に置き換えることで，全体の設計を維持しつつロボットの制御の問題に対処する手法を提案する研究である．

\section{問題（諸言の部分の内容）}

ロボットが走行するに当たって，想定していない事象，例えばタイヤのスリップなどが発生することが起こりうる．そのような想定外の挙動には，その挙動が起きることを踏まえて設計することで対応する工夫による対処が行われているが，起こりうる事象は様々考えられるためすべてを対応しきるには限界がある．\par
それを克服する手段の一つとして，強化学習を適用する考え方がある．強化学習により，数々の経験から最適な行動を行うことで制御を得ることで，起こりうる様々な考慮しきれていない外部要因による影響をも含んだ制御を得ることが可能となり，対応しきれなかった幅広い事象に対応可能な制御方策を獲得する研究が活発に行われている．具体的には，モデルの設計が困難な２足歩行ロボットの制御方策獲得に関する研究
\cite{Actor-Critic型強化学習を用いたヒューマノイドロボットの動作獲得に関する研究}や，深層強化学習によって様々な状況に対応可能な制御を獲得した研究
\cite{深層強化学習を用いた全方位移動ロボットの行動生成手法の開発}，
深層学習によって，まるで意味を理解したような柔軟で人が設計するには困難な動きをロボット自らが獲得した研究
\cite{視覚センサ付き実ロボットによる箱押し行動の獲得}等がある．\par

しかし，強化学習を用いてそのロボットの制御全体を置き換えることは容易ではなく，従来の制御を超える性能を得るために学習パラメータの試行錯誤などが必要となってくる．また，強化学習によって得られる制御は条件反射的な行動が多く，順序だてて複雑な動きを行うような制御を得るのは難しいという特徴\cite{遺伝的プログラミングと強化学習の統合に基づく実ロボットの行動獲得}があった．
そこで，既にある従来の制御手法をもとに，その機能ごとに，強化学習によって得た結果を用いる手法を提案する．
この手法のメリットは，すでにある制御対象の動作の柔軟性向上を効率的に行えることにある，従来の制御手法の一部分を取り替えるため，プログラムを最初から準備する手間が省略できる．そして，強化学習の適用においても，様々な環境に一度に対応するような方策を得ることは難しいが，機能単体ごとに学習させるため学習の行い方を決めやすく，複雑な動作を行うために学習を労力をかけて設計する必要がなく，学習を行いやすい．そして強化学習のメリットである「経験から最適な結果につながるような行動」を得ることができるため，従来の手法で解決が難しかった特徴を，経験により考慮することで解決することが可能になると考えた．
本研究では，その手法を検証する題材として，マイクロマウスでの左右旋回動作を選び，実験を行う．%この題材を選択した理由については後述する．
また，従来の制御としては，公開されているサンプルプログラムをもとに，迷路内をぶつからずに前進し続ける動作を追加したものを考える．
このプログラムは，取り付けられた4つのセンサから壁の状況を判断し，
前進，左旋回，右旋回の行動を選び，各マスごとに移動しながらその行動選択を繰り返すことで前進し続けるというものである．
マスの大きさはすべて決められた値であり等間隔で並んでいるため，パラメータの調整によって，マス一つ分の移動になるように設計されている．
しかしこの制御ではタイヤのスリップなどによる誤差が生じてしまい，
想定通りのマス移動を行うことが前提の従来の制御プログラムでは，誤差がたまり壁にぶつかってしまう問題があった．直進区間がある場合は，左右のセンサの値からマスの中心に戻るように補正をかけることができていたが，それが行える区間のない旋回動作が連続するような箇所では誤差の修正ができず，壁にぶつかってしまう問題が見られた．
本研究では，その問題を提案手法で改善することを目的とし，本手法の有用性を評価する．
よって本研究は，従来の制御方法では未知の要因を含むため対処しきれない問題を，強化学習を用いて得た制御方法に部分的に置き換えることで，全体の設計を維持しつつロボットの制御の問題に対処する手法を提案する研究である，

\section{現状と今後の予定}
PPOによる深層強化学習で，左右の旋回動作を学習させた．\par
台形加減速によって左手法を行わせるプログラムを従来の制御として用意し，その制御の左，右旋回時は学習結果を推論した値によって行動させるものを別に用意する．
その２つで同じコースを走行させる．連続旋回区間（ギザギザの地形）で，５％の確率でタイヤの摩擦を変化させるなどしてスリップを再現した環境を用意し，１００回あたりのゴール率，壁接触率などを比較することで，この手法の有用性を主張する．\par
残っている作業
\begin{itemize}
  \item 台形加減速での左右旋回のゲイン調整
  \item 学習結果を使って左右旋回するプログラム
  \item ランダムにスリップする連続旋回区間の環境の作成
  \item 比較実験
  \item 前刷りの仕上げ
\end{itemize}





\end{document}