{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Gymnasium Mujoco Env\n",
    "参考：[test_mujoco_custom_env.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/tests/envs/mujoco/test_mujoco_custom_env.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オリジナルのMuJoCo環境を作成します\n",
    "\n",
    "オリジナルのMuJoCo環境は，そのクラスを作ることで実装します．\n",
    "実査の学習を行うmain文的な処理は，gymのインターフェースであれば共通で使用できます．\n",
    "よって，自分なりの環境クラスを作成すればいいだけです．\n",
    "その環境クラスには，step, reset, render,などの必須なメソッドを定義する必要がありますが，MuJoCoでは使いやすくするためのサンプルがあります！\n",
    "よってそれを元に作るだけで，強化学習の環境が作れて，学習を回すことができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__credits__ = [\"Kallinteris-Andreas\"]\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "\n",
    "class MouseEnv(MujocoEnv, utils.EzPickle):\n",
    "    \"\"\"\n",
    "    Gymansium.MujocoEnv`環境APIを使った，マイクロマウスの強化学習環境\\n\n",
    "    まずは前進し続ける動作を行う環境を作成します．\\n\n",
    "    mujoco envを継承しています．\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"my_xmls/mouse_in_maze.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = self.data.qpos.size + self.data.qvel.size #たとえば，観測空間に位置と速度を入れたいのであれば，サイズを指定したいので，サイズを取る\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        x_position_before = self.data.qpos[0]\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        x_position_after = self.data.qpos[0]\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward = x_position_after - x_position_before\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def _get_obs(self): #状態空間を取得\n",
    "        position = self.data.qpos.flat.copy()\n",
    "        velocity = self.data.qvel.flat.copy()\n",
    "        return np.concatenate((position, velocity)) #npの値を連結して返す．これが状態空間となる\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos\n",
    "        qvel = self.init_qvel\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "\n",
    "\n",
    "CHECK_ENV_IGNORE_WARNINGS = [\n",
    "    f\"\\x1b[33mWARN: {message}\\x1b[0m\"\n",
    "    for message in [\n",
    "        \"A Box observation space minimum value is -infinity. This is probably too low.\",\n",
    "        \"A Box observation space maximum value is infinity. This is probably too high.\",\n",
    "        \"For Box action spaces, we recommend using a symmetric and normalized space (range=[-1, 1] or [0, 1]). See https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html for more information.\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"frame_skip\", [1, 2, 3, 4, 5])\n",
    "def test_frame_skip(frame_skip):\n",
    "    \"\"\"verify that custom envs work with different `frame_skip` values\"\"\"\n",
    "    env = MouseEnv(frame_skip=frame_skip)\n",
    "\n",
    "    # Test if env adheres to Gym API\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        check_env(env.unwrapped, skip_render_check=True)\n",
    "        env.close()\n",
    "    for warning in w:\n",
    "        if warning.message.args[0] not in CHECK_ENV_IGNORE_WARNINGS:\n",
    "            raise Error(f\"Unexpected warning: {warning.message}\")\n",
    "\n",
    "\n",
    "def test_xml_file():\n",
    "    \"\"\"Verify that the loading of a custom XML file works\"\"\"\n",
    "    relative_path = \"./tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MouseEnv(xml_file=relative_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    full_path = os.getcwd() + \"/tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MouseEnv(xml_file=full_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    # note can not test user home path (with '~') because github CI does not have a home folder\n",
    "\n",
    "\n",
    "def test_reset_info():\n",
    "    \"\"\"Verify that the environment returns info at `reset()`\"\"\"\n",
    "    env = MouseEnv()\n",
    "\n",
    "    _, info = env.reset()\n",
    "    assert info[\"works\"] is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# デベロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__credits__ = [\"Kallinteris-Andreas\"]\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "\n",
    "class MouseEnv(MujocoEnv, utils.EzPickle):\n",
    "    \"\"\"\n",
    "    Gymansium.MujocoEnv`環境APIを使った，マイクロマウスの強化学習環境\\n\n",
    "    まずは前進し続ける動作を行う環境を作成します．\\n\n",
    "    mujoco envを継承しています．\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"my_xmls/mouse_in_maze_for_3_light.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = 14 #self.data.qpos.size + self.data.qvel.size #たとえば，観測空間に位置と速度を入れたいのであれば，サイズを指定したいので，サイズを取る\n",
    "        # self.wall_hit = 0\n",
    "        self.reward_graph = []\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        wall_hit = 0\n",
    "        velocity_before = self.data.sensor(\"Veloci\").data[0] # 前の速度を取る\n",
    "\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        velocity_after = self.data.sensor(\"Veloci\").data[0] # 今の速度を取る\n",
    "        mouse_vel = velocity_after - velocity_before\n",
    "\n",
    "        ls = self.data.sensor('LS').data[0]#sensordata[ls_id]\n",
    "        rs = self.data.sensor('RS').data[0]#sensordata[rs_id]\n",
    "        center_sensor = ls - rs\n",
    "\n",
    "        hit_wall_f = self.data.sensor(\"HB1\").data[0] #フォースセンサの値取得\n",
    "        hit_wall_b = self.data.sensor(\"HB2\").data[0] #フォースセンサの値取得\n",
    "        truncated = False\n",
    "        # print(\"hit wall\", hit_wall_f, hit_wall_b)\n",
    "        if hit_wall_f > 0 or hit_wall_b > 0:\n",
    "            # self.wall_hit = -10\n",
    "            wall_hit = -10\n",
    "            print(\"hit wall\", hit_wall_f, hit_wall_b)\n",
    "            truncated = True\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward = 10*(mouse_vel) + 0*(center_sensor) + wall_hit #self.wall_hitにしないほうが見通しが良い気がする\n",
    "        self.reward_graph.append(reward)\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observation, reward, False, truncated, info\n",
    "\n",
    "    def _get_obs(self): #状態空間を取得\n",
    "        position = self.data.qpos.flat.copy()\n",
    "        velocity = self.data.sensor(\"Veloci\").data[0] # 今の速度を取る\n",
    "        lf = self.data.sensor('LF').data[0]#sensordata[lf_id]\n",
    "        ls = self.data.sensor('LS').data[0]#sensordata[ls_id]\n",
    "        rs = self.data.sensor('RS').data[0]#sensordata[rs_id]\n",
    "        rf = self.data.sensor('RF').data[0]#sensordata[rf_id]\n",
    "        # 0次元配列（スカラー）を1次元配列に変換\n",
    "        velocity = np.array([velocity])\n",
    "        lf = np.array([lf])\n",
    "        ls = np.array([ls])\n",
    "        rs = np.array([rs])\n",
    "        rf = np.array([rf])\n",
    "\n",
    "        # # 結合\n",
    "        # observation = np.concatenate((position, velocity, lf, ls, rs, rf))\n",
    "\n",
    "        return np.concatenate((position, velocity, lf, ls, rs, rf)) #npの値を連結して返す．これが状態空間となる\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos\n",
    "        qvel = self.init_qvel\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "    \n",
    "    # def _get_distance(model, data):\n",
    "    #     lf = data.sensor('LF').data[0]#sensordata[lf_id]\n",
    "    #     ls = data.sensor('LS').data[0]#sensordata[ls_id]\n",
    "    #     rs = data.sensor('RS').data[0]#sensordata[rs_id]\n",
    "    #     rf = data.sensor('RF').data[0]#sensordata[rf_id]\n",
    "    #     return lf,ls,rs,rf\n",
    "\n",
    "\n",
    "CHECK_ENV_IGNORE_WARNINGS = [\n",
    "    f\"\\x1b[33mWARN: {message}\\x1b[0m\"\n",
    "    for message in [\n",
    "        \"A Box observation space minimum value is -infinity. This is probably too low.\",\n",
    "        \"A Box observation space maximum value is infinity. This is probably too high.\",\n",
    "        \"For Box action spaces, we recommend using a symmetric and normalized space (range=[-1, 1] or [0, 1]). See https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html for more information.\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"frame_skip\", [1, 2, 3, 4, 5])\n",
    "def test_frame_skip(frame_skip):\n",
    "    \"\"\"verify that custom envs work with different `frame_skip` values\"\"\"\n",
    "    env = MouseEnv(frame_skip=frame_skip)\n",
    "\n",
    "    # Test if env adheres to Gym API\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        check_env(env.unwrapped, skip_render_check=True)\n",
    "        env.close()\n",
    "    for warning in w:\n",
    "        if warning.message.args[0] not in CHECK_ENV_IGNORE_WARNINGS:\n",
    "            raise Error(f\"Unexpected warning: {warning.message}\")\n",
    "\n",
    "\n",
    "def test_xml_file():\n",
    "    \"\"\"Verify that the loading of a custom XML file works\"\"\"\n",
    "    relative_path = \"./tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MouseEnv(xml_file=relative_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    full_path = os.getcwd() + \"/tests/envs/mujoco/assets/walker2d_v5_uneven_feet.xml\"\n",
    "    env = MouseEnv(xml_file=full_path).unwrapped\n",
    "    assert isinstance(env, MujocoEnv)\n",
    "    assert env.data.qpos.size == 9\n",
    "\n",
    "    # note can not test user home path (with '~') because github CI does not have a home folder\n",
    "\n",
    "\n",
    "def test_reset_info():\n",
    "    \"\"\"Verify that the environment returns info at `reset()`\"\"\"\n",
    "    env = MouseEnv()\n",
    "\n",
    "    _, info = env.reset()\n",
    "    assert info[\"works\"] is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "gym.envs.registration.register(id='MouseEnv-v0',entry_point=MouseEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymのインターフェースになっているかチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:441: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = MouseEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cudaが使えるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.24e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 29        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.26e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040799323 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00199      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.29e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 9.21e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.2e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004043991 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.14e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.52e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.18e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023918208 |\n",
      "|    clip_fraction        | 0.00659      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.55e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.09e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -1.18e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 29            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 348           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041029602 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0107        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.47e+03      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000285     |\n",
      "|    std                  | 0.982         |\n",
      "|    value_loss           | 7.74e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.19e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002999156 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.00643     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.26e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 7.02e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.2e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 488          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018341511 |\n",
      "|    clip_fraction        | 0.00674      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.00308      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 9.1e+03      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# model = PPO.load('test',env)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# callback=ProgressCallback())\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# モデルのテスト\u001b[39;00m\n\u001b[0;32m     38\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# ここを修正\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:143\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([newth, newthdot])\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), \u001b[38;5;241m-\u001b[39mcosts, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:259\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    258\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"ALE/Breakout-v5\",  render_mode=\"human\")\n",
    "# env = gym.make(\"CartPole-v1\",  render_mode=\"human\")\n",
    "# # env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)　こんな感じで，\n",
    "# model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=200)\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ProgressCallback, self).__init__(verbose)\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # エピソードが終了したかどうかをチェック\n",
    "        if 'episode' in self.locals:\n",
    "            self.episode_count += 1\n",
    "            print(f\"ああああEpisode: {self.episode_count}, Reward: {self.locals['episode']['r']}\")\n",
    "        return True\n",
    "\n",
    "# # 使用例\n",
    "# model.learn(total_timesteps=5000, callback=ProgressCallback())\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\",  render_mode=\"human\")\n",
    "# env.model.opt.timestep = 0.01  # タイムステップを設定\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# model = PPO.load('test',env)\n",
    "model.learn(total_timesteps=20000,)# callback=ProgressCallback())\n",
    "\n",
    "\n",
    "\n",
    "# モデルのテスト\n",
    "state, _ = env.reset()  # ここを修正\n",
    "for i in trange(600):\n",
    "    # 環境の描画\n",
    "    env.render()\n",
    "\n",
    "    # モデルの推論\n",
    "    action, _ = model.predict(state)\n",
    "\n",
    "    # 1ステップ実行\n",
    "    obs, reward, terminated, truncated, info = env.step(action) #以前の4要素のタプルから5要素のタプルに変更され，`observation, reward, terminated, truncated, info`という形式になった。\n",
    "\n",
    "\n",
    "\n",
    "    # エピソード完了（終了または切り捨て）のチェック\n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()  # エピソードが終了したら、環境をリセット\n",
    "# モデルの保存 (1)\n",
    "model.save('test')\n",
    "plt.title(\"1マス前進\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"step(s)\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"速度[m/s]\", fontname=\"MS Gothic\")\n",
    "\n",
    "#前進関係のプロット\n",
    "t = list(range(len(env.reward_graph)))  # 0から始まるインデックスのリストを作成\n",
    "# plt.plot(t, env.reward_graph, linestyle='solid', label=\"報酬\")\n",
    "plt.legend()\n",
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.step(action)の戻り値の更新について（４つから５つになりました）\n",
    "Gymライブラリのバージョン0.26以降、及びGymnasiumの全バージョンにおいて、`env.step()`メソッドの戻り値は、以前の4要素のタプルから5要素のタプルに変更されました。これまでの`observation, reward, done, info`という形式から、`observation, reward, terminated, truncated, info`という形式になっています。\n",
    "\n",
    "ここでの`terminated`と`truncated`はどちらもブール値（真偽値）です。`terminated`は環境がマルコフ決定過程（MDP）のタスクの定義に基づいて終了状態に達したことを示します。これは通常、環境の目標が達成されたり、終了条件が満たされたときに発生します。一方、`truncated`は、エピソードがMDPの範囲外の条件、たとえば時間制限や境界条件（エージェントが境界を越えるなど）によって終了したことを示します。強化学習アルゴリズムにおいて、特にブートストラップを伴うものでは、終了と切り捨て状態の取り扱いが値の更新において異なる場合があるため、この区別は重要です。\n",
    "\n",
    "Gymライブラリのこの更新は、強化学習タスクにおける終了と切り捨ての明確な区別を提供するために行われました。これは、特定のアルゴリズムの正確な実装にとって重要です。終了(`terminated`)と切り捨て(`truncated`)の両方のタイプのエピソード終了を区別するために、両方が含まれるようになりました。\n",
    "\n",
    "あなたの目的に合わせて、エピソードが終了したかどうか（理由に関係なく）を知りたい場合は、`terminated`または`truncated`のいずれかが`True`であるかどうかを確認すればよいです。その場合、環境をリセットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのテスト\n",
    "state, _ = env.reset()  # ここを修正\n",
    "for i in trange(4000):\n",
    "    # 環境の描画\n",
    "    env.render()\n",
    "\n",
    "    # モデルの推論\n",
    "    action, _ = model.predict(state)\n",
    "\n",
    "    # 1ステップ実行\n",
    "    state, rewards, done, info = env.step(action)\n",
    "\n",
    "    # エピソード完了\n",
    "    if done:\n",
    "        state, _ = env.reset()  # エピソードが終了したら、環境をリセット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_string_example():\n",
    "        \"\"\"ドックストリング（Docstring）\n",
    "        この関数は以下のことを行います：\\n\n",
    "    - param1を処理します。\\n\n",
    "    - param2を別の方法で処理します。\\n\n",
    "    最後に、結果を返します。\n",
    "        シミュレーションをnフレーム進め、制御アクションを適用する。\n",
    "        Step the simulation n number of frames and applying a control action.\n",
    "        \"\"\"\n",
    "        p = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
