{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\atusi\\appdata\\local\\temp\\pip-req-build-_nkcdc02\n",
      "  Resolved https://github.com/openai/whisper.git to commit b91c907694f96a3fb9da03d4bbdc83fbcd3a40a4\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tiktoken==0.3.3\n",
      "  Downloading tiktoken-0.3.3-cp39-cp39-win_amd64.whl (579 kB)\n",
      "     -------------------------------------- 579.8/579.8 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from openai-whisper==20230314) (2.0.0+cu117)\n",
      "Requirement already satisfied: tqdm in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from openai-whisper==20230314) (4.65.0)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-10.0.0-py3-none-any.whl (55 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.57.1-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from openai-whisper==20230314) (1.24.1)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2023.6.3-cp39-cp39-win_amd64.whl (268 kB)\n",
      "     ---------------------------------------- 268.1/268.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.28.1)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Downloading llvmlite-0.40.1-cp39-cp39-win_amd64.whl (27.7 MB)\n",
      "     ---------------------------------------- 27.7/27.7 MB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from torch->openai-whisper==20230314) (3.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from torch->openai-whisper==20230314) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from tqdm->openai-whisper==20230314) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from sympy->torch->openai-whisper==20230314) (1.2.1)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=807593 sha256=089aeabb3efaf5fa865aff476a22d67ad6257bc16e38e9e15fc9887bef635b1a\n",
      "  Stored in directory: C:\\Users\\atusi\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-8ce2kh3o\\wheels\\fe\\03\\29\\e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: regex, more-itertools, llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.40.1 more-itertools-10.0.0 numba-0.57.1 openai-whisper-20230314 regex-2023.6.3 tiktoken-0.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\atusi\\AppData\\Local\\Temp\\pip-req-build-_nkcdc02'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 指定されたファイルが見つかりません。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranscribe(\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdvd.mp3\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\whisper\\transcribe.py:121\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[0;32m    118\u001b[0m     decode_options[\u001b[39m\"\u001b[39m\u001b[39mfp16\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m mel \u001b[39m=\u001b[39m log_mel_spectrogram(audio, padding\u001b[39m=\u001b[39;49mN_SAMPLES)\n\u001b[0;32m    122\u001b[0m content_frames \u001b[39m=\u001b[39m mel\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m N_FRAMES\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m decode_options\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\whisper\\audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[1;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(audio):\n\u001b[0;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(audio, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m         audio \u001b[39m=\u001b[39m load_audio(audio)\n\u001b[0;32m    141\u001b[0m     audio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(audio)\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\whisper\\audio.py:59\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# fmt: on\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     out \u001b[39m=\u001b[39m run(cmd, capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mstdout\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m CalledProcessError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load audio: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m    503\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[1;32m--> 505\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[0;32m    506\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    948\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    952\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    953\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    954\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    955\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    956\u001b[0m                         errread, errwrite,\n\u001b[0;32m    957\u001b[0m                         restore_signals,\n\u001b[0;32m    958\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    959\u001b[0m                         start_new_session)\n\u001b[0;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1421\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1423\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1424\u001b[0m                              creationflags,\n\u001b[0;32m   1425\u001b[0m                              env,\n\u001b[0;32m   1426\u001b[0m                              cwd,\n\u001b[0;32m   1427\u001b[0m                              startupinfo)\n\u001b[0;32m   1428\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1436\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1437\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 指定されたファイルが見つかりません。"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(\"C:\\dvd.mp3\", verbose=True)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# いくぞ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faster-whisper\n",
      "  Downloading faster_whisper-0.7.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.0 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "     ---------------------------------------- 7.5/7.5 MB 11.7 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ------------------------------------- 977.6/977.6 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting onnxruntime<2,>=1.14\n",
      "  Downloading onnxruntime-1.15.1-cp39-cp39-win_amd64.whl (6.7 MB)\n",
      "     ---------------------------------------- 6.7/6.7 MB 9.0 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.13\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     -------------------------------------- 268.8/268.8 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting ctranslate2<4,>=3.17\n",
      "  Downloading ctranslate2-3.18.0-cp39-cp39-win_amd64.whl (20.1 MB)\n",
      "     --------------------------------------- 20.1/20.1 MB 11.1 MB/s eta 0:00:00\n",
      "Collecting tokenizers==0.13.*\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting av==10.*\n",
      "  Downloading av-10.0.0-cp39-cp39-win_amd64.whl (25.3 MB)\n",
      "     --------------------------------------- 25.3/25.3 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-win_amd64.whl (266 kB)\n",
      "     -------------------------------------- 266.4/266.4 kB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from huggingface-hub>=0.13->faster-whisper) (4.5.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from huggingface-hub>=0.13->faster-whisper) (2023.4.0)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.11.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.2.1)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.2/95.2 kB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pyreadline3, av, humanfriendly, ctranslate2, huggingface-hub, coloredlogs, transformers, onnxruntime, faster-whisper\n",
      "Successfully installed av-10.0.0 coloredlogs-15.0.1 ctranslate2-3.18.0 faster-whisper-0.7.1 huggingface-hub-0.16.4 humanfriendly-10.0 onnxruntime-1.15.1 pyreadline3-3.4.1 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.32.0\n",
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages (from yt-dlp) (2022.12.7)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp39-cp39-win_amd64.whl (383 kB)\n",
      "     ------------------------------------- 383.4/383.4 kB 12.0 MB/s eta 0:00:00\n",
      "Collecting pycryptodomex\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 11.1 MB/s eta 0:00:00\n",
      "Collecting websockets\n",
      "  Downloading websockets-11.0.3-cp39-cp39-win_amd64.whl (124 kB)\n",
      "     -------------------------------------- 124.7/124.7 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting mutagen\n",
      "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
      "     -------------------------------------- 193.6/193.6 kB 5.9 MB/s eta 0:00:00\n",
      "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
      "Successfully installed brotli-1.0.9 mutagen-1.46.0 pycryptodomex-3.18.0 websockets-11.0.3 yt-dlp-2023.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install faster-whisper transformers sentencepiece\n",
    "!python -m pip install -U yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/live/outcGtbnMuQ?feature=share\n",
      "[youtube] outcGtbnMuQ: Downloading webpage\n",
      "[youtube] outcGtbnMuQ: Downloading ios player API JSON\n",
      "[youtube] outcGtbnMuQ: Downloading android player API JSON\n",
      "[youtube] outcGtbnMuQ: Downloading m3u8 information\n",
      "[info] outcGtbnMuQ: Downloading 1 format(s): 251\n",
      "[download] Destination: audio.webm\n",
      "\n",
      "[download]   0.0% of   17.93MiB at  190.27KiB/s ETA 01:36\n",
      "[download]   0.0% of   17.93MiB at  478.64KiB/s ETA 00:38\n",
      "[download]   0.0% of   17.93MiB at    1.09MiB/s ETA 00:16\n",
      "[download]   0.1% of   17.93MiB at    2.02MiB/s ETA 00:08\n",
      "[download]   0.2% of   17.93MiB at    1.61MiB/s ETA 00:11\n",
      "[download]   0.3% of   17.93MiB at    1.87MiB/s ETA 00:09\n",
      "[download]   0.7% of   17.93MiB at    2.66MiB/s ETA 00:06\n",
      "[download]   1.4% of   17.93MiB at    3.75MiB/s ETA 00:04\n",
      "[download]   2.8% of   17.93MiB at    5.40MiB/s ETA 00:03\n",
      "[download]   5.6% of   17.93MiB at    6.04MiB/s ETA 00:02\n",
      "[download]  11.2% of   17.93MiB at    8.64MiB/s ETA 00:01\n",
      "[download]  22.3% of   17.93MiB at    9.40MiB/s ETA 00:01\n",
      "[download]  44.6% of   17.93MiB at   10.40MiB/s ETA 00:00\n",
      "[download]  55.1% of   17.93MiB at   10.49MiB/s ETA 00:00\n",
      "[download]  55.1% of   17.93MiB at  212.36KiB/s ETA 00:38\n",
      "[download]  55.1% of   17.93MiB at  574.85KiB/s ETA 00:14\n",
      "[download]  55.2% of   17.93MiB at    1.31MiB/s ETA 00:06\n",
      "[download]  55.2% of   17.93MiB at    2.81MiB/s ETA 00:02\n",
      "[download]  55.3% of   17.93MiB at    1.75MiB/s ETA 00:04\n",
      "[download]  55.5% of   17.93MiB at    2.07MiB/s ETA 00:03\n",
      "[download]  55.8% of   17.93MiB at    2.66MiB/s ETA 00:02\n",
      "[download]  56.5% of   17.93MiB at    3.97MiB/s ETA 00:01\n",
      "[download]  57.9% of   17.93MiB at    5.77MiB/s ETA 00:01\n",
      "[download]  60.7% of   17.93MiB at    6.85MiB/s ETA 00:01\n",
      "[download]  66.3% of   17.93MiB at    8.18MiB/s ETA 00:00\n",
      "[download]  77.4% of   17.93MiB at    9.34MiB/s ETA 00:00\n",
      "[download]  99.7% of   17.93MiB at   10.54MiB/s ETA 00:00\n",
      "[download] 100.0% of   17.93MiB at   10.56MiB/s ETA 00:00\n",
      "[download] 100% of   17.93MiB in 00:00:01 at 9.78MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Postprocessing: ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location\n"
     ]
    }
   ],
   "source": [
    "!yt-dlp -x --audio-format mp3 https://www.youtube.com/live/outcGtbnMuQ?feature=share -o audio.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 0.998047\n",
      "[0.000000s -> 12.360000s]  to the GPT-4 developer demo livestream.\n",
      "[12.360000s -> 13.800000s]  Honestly, it's kind of hard for me\n",
      "[13.800000s -> 15.400000s]  to believe that this day is here.\n",
      "[15.400000s -> 18.280000s]  OpenAI has been building this technology really\n",
      "[18.280000s -> 19.680000s]  since we started the company.\n",
      "[19.680000s -> 21.080000s]  But for the past two years, we've\n",
      "[21.080000s -> 24.080000s]  been really focused on delivering GPT-4.\n",
      "[24.080000s -> 27.420000s]  That started with rebuilding our entire training stack,\n",
      "[27.420000s -> 29.560000s]  actually training the model, and then\n",
      "[29.560000s -> 31.980000s]  seeing what it was capable of, trying to figure out\n",
      "[31.980000s -> 34.660000s]  its capabilities, its risks, working with partners\n",
      "[34.660000s -> 37.060000s]  in order to test it in real-world scenarios,\n",
      "[37.060000s -> 39.820000s]  really tuning its behavior, optimizing the model,\n",
      "[39.820000s -> 42.820000s]  getting it available so that you can use it.\n",
      "[42.820000s -> 45.340000s]  And so today, our goal is to show you\n",
      "[45.340000s -> 48.460000s]  a little bit of how to make GPT-4 shine,\n",
      "[48.460000s -> 52.300000s]  how to really get the most out of it, where its weaknesses are,\n",
      "[52.300000s -> 54.880000s]  where we're still working on it, and just how to really use it\n",
      "[54.880000s -> 57.100000s]  as a good tool, a good partner.\n",
      "[57.120000s -> 60.800000s]  So if you're interested in participating in the stream,\n",
      "[60.800000s -> 63.120000s]  if you go to our Discord, so that's discord.gg slash\n",
      "[63.120000s -> 64.880000s]  OpenAI, there's comments in there,\n",
      "[64.880000s -> 68.440000s]  and we'll take a couple of audience suggestions.\n",
      "[68.440000s -> 70.060000s]  So the first thing I want to show you\n",
      "[70.060000s -> 74.160000s]  is the first task that GPT-4 could do that we never really\n",
      "[74.160000s -> 76.080000s]  got 3.5 to do.\n",
      "[76.080000s -> 78.460000s]  And the way to think about this is all throughout training\n",
      "[78.460000s -> 80.600000s]  that you're constantly doing all this work.\n",
      "[80.600000s -> 81.480000s]  It's 2 AM.\n",
      "[81.480000s -> 82.280000s]  The pager goes off.\n",
      "[82.280000s -> 83.760000s]  You fix the model.\n",
      "[83.760000s -> 87.260000s]  And you're always wondering, is it going to work?\n",
      "[87.260000s -> 89.660000s]  Is all of this effort actually going to pan out?\n",
      "[89.660000s -> 92.620000s]  And so we all had a pet task that we really liked\n",
      "[92.620000s -> 95.100000s]  and that we would all individually be trying to see,\n",
      "[95.100000s -> 97.140000s]  is the model capable of it now?\n",
      "[97.140000s -> 99.660000s]  And I'm going to show you the first one\n",
      "[99.660000s -> 103.800000s]  that we had a success for 4, but never really got there for 3.5.\n",
      "[103.800000s -> 106.540000s]  So I'm just going to copy the top of our blog post from today.\n",
      "[106.540000s -> 109.900000s]  Going to paste it into our playground.\n",
      "[109.900000s -> 112.620000s]  Now, this is our new chat completions playground\n",
      "[112.640000s -> 114.040000s]  that came out two weeks ago.\n",
      "[114.040000s -> 116.600000s]  I'm going to show you first with GPT-3.5.\n",
      "[116.600000s -> 119.560000s]  4 has the same API to it, the same playground.\n",
      "[119.560000s -> 121.880000s]  The way that it works is you have a system message where\n",
      "[121.880000s -> 125.360000s]  you explain to the model what it's supposed to do.\n",
      "[125.360000s -> 127.120000s]  And we've made these models very steerable.\n",
      "[127.120000s -> 129.320000s]  So you can provide it with really any instruction you\n",
      "[129.320000s -> 130.880000s]  want, whatever you dream up.\n",
      "[130.880000s -> 132.800000s]  And the model will adhere to it pretty well.\n",
      "[132.800000s -> 134.680000s]  And in the future, it will get increasingly,\n",
      "[134.680000s -> 137.600000s]  increasingly powerful at steering the model very\n",
      "[137.600000s -> 140.440000s]  reliably.\n",
      "[140.440000s -> 142.700000s]  You can then paste whatever you want as a user.\n",
      "[142.700000s -> 144.880000s]  The model will return messages as an assistant.\n",
      "[144.880000s -> 146.460000s]  And the way to think of it is that we're\n",
      "[146.460000s -> 149.460000s]  moving away from just raw text in, raw text out,\n",
      "[149.460000s -> 151.220000s]  where you can't tell where different parts\n",
      "[151.220000s -> 153.380000s]  of the conversation come from, but towards this much more\n",
      "[153.380000s -> 156.620000s]  structured format that gives the model the opportunity to know,\n",
      "[156.620000s -> 158.620000s]  well, this is the user asking me to do something\n",
      "[158.620000s -> 160.140000s]  that the developer didn't intend.\n",
      "[160.140000s -> 162.700000s]  I should listen to the developer here.\n",
      "[162.700000s -> 164.700000s]  All right, so now, time to actually show you\n",
      "[164.700000s -> 166.140000s]  the task I'm referring to.\n",
      "[166.140000s -> 167.620000s]  So everyone's familiar with?\n",
      "[167.620000s -> 169.300000s]  Summarize.\n",
      "[169.320000s -> 173.160000s]  This is an article into a sentence,\n",
      "[173.160000s -> 175.760000s]  getting a little more specific, but where\n",
      "[175.760000s -> 179.960000s]  every word begins with G. So this is 3.5.\n",
      "[179.960000s -> 182.440000s]  Let's see what it does.\n",
      "[182.440000s -> 184.520000s]  Yeah, it kind of didn't even try.\n",
      "[184.520000s -> 185.920000s]  Just gave up on the task.\n",
      "[185.920000s -> 188.240000s]  This is pretty typical for 3.5 trying\n",
      "[188.240000s -> 190.520000s]  to do this particular kind of task.\n",
      "[190.520000s -> 194.200000s]  If it's sort of a very kind of stilted article or something\n",
      "[194.200000s -> 195.720000s]  like that, maybe it can succeed.\n",
      "[195.740000s -> 199.420000s]  But for the most part, 3.5 just gives up.\n",
      "[199.420000s -> 203.820000s]  But let's try the exact same prompt, the exact same system\n",
      "[203.820000s -> 206.620000s]  message in GPT-4.\n",
      "[209.540000s -> 212.780000s]  So kind of borderline whether you want to count AI or not,\n",
      "[212.780000s -> 216.580000s]  but so let's say AI doesn't count.\n",
      "[216.580000s -> 217.180000s]  That's cheating.\n",
      "[221.660000s -> 222.480000s]  So fair enough.\n",
      "[222.480000s -> 224.860000s]  The model happily accepts my feedback.\n",
      "[224.880000s -> 226.960000s]  So now to make sure it's not just good for Gs,\n",
      "[226.960000s -> 228.720000s]  I'd like to turn this over to the audience.\n",
      "[228.720000s -> 231.280000s]  I'll take a suggestion on what letter to try next.\n",
      "[231.280000s -> 233.440000s]  In the meanwhile, while I'm waiting for our moderators\n",
      "[233.440000s -> 242.360000s]  to pick the lucky, lucky letter, I will give a try with A.\n",
      "[242.360000s -> 244.800000s]  But in this case, I'll say GPT-4 is fine.\n",
      "[244.800000s -> 247.800000s]  Why not?\n",
      "[247.800000s -> 249.680000s]  Also, pretty good summary.\n",
      "[249.680000s -> 252.120000s]  So I'll hop over to our Discord.\n",
      "[252.120000s -> 253.400000s]  All right.\n",
      "[253.400000s -> 254.240000s]  Wow.\n",
      "[254.240000s -> 256.020000s]  People are being a little ambitious here.\n",
      "[256.020000s -> 258.220000s]  I'm really trying to put the model through the paces.\n",
      "[258.220000s -> 260.900000s]  We're going to try Q, which if you think about this\n",
      "[260.900000s -> 263.360000s]  for a moment, I want the audience to really think about\n",
      "[263.360000s -> 266.060000s]  how would you do a summary of this article that\n",
      "[266.060000s -> 268.020000s]  all starts with Q. It's not easy.\n",
      "[277.340000s -> 278.740000s]  It's pretty good.\n",
      "[278.740000s -> 280.340000s]  That's pretty good.\n",
      "[280.340000s -> 280.860000s]  All right.\n",
      "[280.880000s -> 285.240000s]  So I've shown you summarizing an existing article.\n",
      "[285.240000s -> 288.160000s]  I want to show you how you can flexibly combine ideas\n",
      "[288.160000s -> 289.860000s]  between different articles.\n",
      "[289.860000s -> 291.600000s]  So I'm going to take this article that\n",
      "[291.600000s -> 296.920000s]  was on Hacker News yesterday, copy-paste it\n",
      "[296.920000s -> 299.440000s]  into the same conversation so it has all the context of what\n",
      "[299.440000s -> 300.400000s]  we were just doing.\n",
      "[300.400000s -> 303.960000s]  I'm going to say, find one common theme\n",
      "[303.960000s -> 307.160000s]  between this article and the GPT-4 blog.\n",
      "[310.860000s -> 313.120000s]  So this is an article about Pinecone,\n",
      "[313.120000s -> 315.640000s]  which is a Python web app development framework.\n",
      "[315.640000s -> 318.440000s]  And it's making the technology more accessible, user-friendly.\n",
      "[318.440000s -> 319.640000s]  If you don't think that was insightful enough,\n",
      "[319.640000s -> 321.020000s]  you can always give some feedback\n",
      "[321.020000s -> 326.560000s]  and say, that was not insightful enough.\n",
      "[326.560000s -> 328.920000s]  Please, no, I'll just even just leave it there.\n",
      "[328.920000s -> 330.680000s]  Leave it up to the model to decide.\n",
      "[330.680000s -> 332.640000s]  So bridging the gap between powerful technology\n",
      "[332.640000s -> 334.200000s]  and practical applications.\n",
      "[334.200000s -> 335.520000s]  Seems not bad.\n",
      "[335.520000s -> 337.960000s]  And of course, you can ask for any other kind of task\n",
      "[337.980000s -> 340.060000s]  you want using its flexible language\n",
      "[340.060000s -> 341.940000s]  understanding and synthesis.\n",
      "[341.940000s -> 347.460000s]  You can ask for something like, now turn the GPT-4 blog post\n",
      "[347.460000s -> 349.380000s]  into a rhyming poem.\n",
      "[358.460000s -> 360.940000s]  Picked up on OpenAI evals, open source for all,\n",
      "[360.940000s -> 362.540000s]  helping to guide answering the call.\n",
      "[362.540000s -> 365.100000s]  Which by the way, if you'd like to contribute to this model,\n",
      "[365.100000s -> 366.740000s]  please give us evals.\n",
      "[366.740000s -> 368.440000s]  We have an open source evaluation framework\n",
      "[368.440000s -> 370.280000s]  that will help us guide and all of our users\n",
      "[370.280000s -> 372.920000s]  understand what the model is capable of\n",
      "[372.920000s -> 375.280000s]  and to take it to the next level.\n",
      "[375.280000s -> 376.080000s]  So there we go.\n",
      "[376.080000s -> 379.840000s]  This is consuming existing content using GPT-4\n",
      "[379.840000s -> 382.960000s]  with a little bit of creativity on top.\n",
      "[382.960000s -> 387.200000s]  But next, I want to show you how to build with GPT-4.\n",
      "[387.200000s -> 391.120000s]  What it's like to create with it as a partner.\n",
      "[391.120000s -> 392.900000s]  And so the thing we're going to do\n",
      "[392.900000s -> 396.920000s]  is we're going to actually build a Discord bot.\n",
      "[396.920000s -> 398.920000s]  I'll build it live and show you the process,\n",
      "[398.920000s -> 400.440000s]  show you debugging, show you what\n",
      "[400.440000s -> 402.480000s]  the model can do, where its limitations are,\n",
      "[402.480000s -> 404.200000s]  and how to work with them in order\n",
      "[404.200000s -> 405.680000s]  to achieve new heights.\n",
      "[405.680000s -> 407.520000s]  So the first thing I'll do is tell the model\n",
      "[407.520000s -> 410.560000s]  that this time, it's supposed to be an AI programming\n",
      "[410.560000s -> 411.720000s]  assistant.\n",
      "[411.720000s -> 414.400000s]  Its job is to write things out in pseudocode first\n",
      "[414.400000s -> 416.280000s]  and then actually write the code.\n",
      "[416.280000s -> 418.600000s]  And this approach is very helpful\n",
      "[418.600000s -> 422.160000s]  to let the model break down the problem into smaller pieces.\n",
      "[422.180000s -> 424.100000s]  And then that way, you're not asking it\n",
      "[424.100000s -> 427.620000s]  to just come up with a super hard solution to a problem\n",
      "[427.620000s -> 429.100000s]  all in one go.\n",
      "[429.100000s -> 430.500000s]  It also makes it very interpretable\n",
      "[430.500000s -> 433.020000s]  because you can see exactly what the model was thinking.\n",
      "[433.020000s -> 435.460000s]  And you can even provide corrections if you'd like.\n",
      "[435.460000s -> 439.940000s]  So here is the prompt that we're going to ask it.\n",
      "[439.940000s -> 442.380000s]  This is the kind of thing that 3.5 would totally choke on\n",
      "[442.380000s -> 443.940000s]  if you've tried anything like it.\n",
      "[443.940000s -> 446.100000s]  But so we're going to ask for a Discord bot that\n",
      "[446.100000s -> 451.460000s]  uses the GPT-4 API to read images and text.\n",
      "[451.480000s -> 453.400000s]  Now, there's one problem here, which\n",
      "[453.400000s -> 457.640000s]  is this model's training cutoff is in 2021,\n",
      "[457.640000s -> 460.800000s]  which means it has not seen our new chat completions format.\n",
      "[460.800000s -> 463.680000s]  So I literally just went to the blog post from two weeks ago,\n",
      "[463.680000s -> 467.060000s]  copy pasted from the blog post, including the response format.\n",
      "[467.060000s -> 469.760000s]  It has not seen the new image extension to that.\n",
      "[469.760000s -> 473.920000s]  And so I just kind of wrote that up in just very minimal detail\n",
      "[473.920000s -> 476.000000s]  about how to include images.\n",
      "[476.000000s -> 479.600000s]  And now, the model can actually leverage that documentation\n",
      "[479.820000s -> 482.180000s]  it did not have memorized, that it does not know.\n",
      "[489.900000s -> 493.340000s]  And in general, these models are very good at using information\n",
      "[493.340000s -> 495.220000s]  that it's been trained on in new ways\n",
      "[495.220000s -> 496.700000s]  and synthesizing new content.\n",
      "[496.700000s -> 498.700000s]  And you can see that right here, that it actually\n",
      "[498.700000s -> 501.460000s]  wrote an entirely new bot.\n",
      "[501.460000s -> 504.540000s]  Now, let's actually see if this bot\n",
      "[504.540000s -> 506.060000s]  is going to work in practice.\n",
      "[506.060000s -> 508.220000s]  So you should always look through the code\n",
      "[508.220000s -> 509.500000s]  to get a sense of what it does.\n",
      "[509.500000s -> 513.800000s]  Don't run untrusted code from humans or from AIs.\n",
      "[513.800000s -> 516.800000s]  And one thing to note is that the Discord API has\n",
      "[516.800000s -> 520.080000s]  changed a lot over time, and particularly\n",
      "[520.080000s -> 522.400000s]  that there's one feature that has changed a lot\n",
      "[522.400000s -> 525.000000s]  since this model was trained.\n",
      "[525.000000s -> 526.160000s]  Give it a try.\n",
      "[526.160000s -> 529.360000s]  In fact, yes, we are missing the intense keyword.\n",
      "[529.360000s -> 532.200000s]  This is something that came out in 2020.\n",
      "[532.200000s -> 533.600000s]  So the model does know it exists,\n",
      "[533.600000s -> 535.840000s]  but it doesn't know which version of the Discord\n",
      "[535.840000s -> 537.360000s]  API we're using.\n",
      "[537.360000s -> 538.680000s]  So are we out of luck?\n",
      "[538.700000s -> 540.300000s]  Well, not quite.\n",
      "[540.300000s -> 543.180000s]  We can just simply paste to the model exactly the error\n",
      "[543.180000s -> 545.060000s]  message, not even going to say, hey,\n",
      "[545.060000s -> 546.420000s]  this is from running your code.\n",
      "[546.420000s -> 548.500000s]  Could you please fix it?\n",
      "[548.500000s -> 550.580000s]  We'll just let it run.\n",
      "[550.580000s -> 551.780000s]  And the model says, oh, yeah.\n",
      "[551.780000s -> 553.020000s]  Whoops, the intense argument.\n",
      "[553.020000s -> 554.580000s]  Here's the correct code.\n",
      "[557.980000s -> 560.540000s]  Now, let's give this a try, once again kind of making sure\n",
      "[560.540000s -> 562.380000s]  that we understand what the code is doing.\n",
      "[562.680000s -> 564.720000s]  Now, a second issue that can come up\n",
      "[564.720000s -> 567.120000s]  is it doesn't know what environment I'm running in.\n",
      "[567.120000s -> 569.720000s]  And if you notice, it says, hey, here's\n",
      "[569.720000s -> 572.160000s]  this inscrutable error message, which\n",
      "[572.160000s -> 574.560000s]  if you've not used Jupyter Notebook a lot with async IO\n",
      "[574.560000s -> 577.960000s]  before, you probably have no idea what this means.\n",
      "[577.960000s -> 584.080000s]  But fortunately, once again, you can just sort of say to the model,\n",
      "[584.080000s -> 590.960000s]  hey, I am using Jupyter, and I'm running in the wrong environment.\n",
      "[591.940000s -> 596.300000s]  And I would like to make this work.\n",
      "[596.300000s -> 598.660000s]  And you fix it.\n",
      "[598.660000s -> 600.740000s]  And the specific problem is that there's already\n",
      "[600.740000s -> 601.660000s]  an event loop running.\n",
      "[601.660000s -> 605.060000s]  So you need to use this nest async IO library.\n",
      "[605.060000s -> 607.540000s]  You need to call nest async IO.apply.\n",
      "[607.540000s -> 610.460000s]  The model knows all of this, correctly instantiates\n",
      "[610.460000s -> 613.740000s]  all of these pieces into the bot.\n",
      "[613.740000s -> 616.140000s]  It even helpfully tells you, oh, you're running in Jupyter.\n",
      "[616.140000s -> 617.980000s]  Well, you can do this bang pip install\n",
      "[617.980000s -> 620.740000s]  in order to install the package if you don't already have it.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m segments, info \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtranscribe(\u001b[39m\"\u001b[39m\u001b[39maudio.webm\u001b[39m\u001b[39m\"\u001b[39m, beam_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDetected language \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with probability \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (info\u001b[39m.\u001b[39mlanguage, info\u001b[39m.\u001b[39mlanguage_probability))\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m segment \u001b[39min\u001b[39;00m segments:\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%.6f\u001b[39;00m\u001b[39ms -> \u001b[39m\u001b[39m%.6f\u001b[39;00m\u001b[39ms] \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (segment\u001b[39m.\u001b[39mstart, segment\u001b[39m.\u001b[39mend, segment\u001b[39m.\u001b[39mtext))\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\faster_whisper\\transcribe.py:403\u001b[0m, in \u001b[0;36mWhisperModel.generate_segments\u001b[1;34m(self, features, tokenizer, options, encoder_output)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m encoder_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(segment)\n\u001b[0;32m    398\u001b[0m (\n\u001b[0;32m    399\u001b[0m     result,\n\u001b[0;32m    400\u001b[0m     avg_logprob,\n\u001b[0;32m    401\u001b[0m     temperature,\n\u001b[0;32m    402\u001b[0m     compression_ratio,\n\u001b[1;32m--> 403\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_with_fallback(encoder_output, prompt, tokenizer, options)\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m options\u001b[39m.\u001b[39mno_speech_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     \u001b[39m# no voice activity check\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     should_skip \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mno_speech_prob \u001b[39m>\u001b[39m options\u001b[39m.\u001b[39mno_speech_threshold\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\faster_whisper\\transcribe.py:604\u001b[0m, in \u001b[0;36mWhisperModel.generate_with_fallback\u001b[1;34m(self, encoder_output, prompt, tokenizer, options)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    600\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbeam_size\u001b[39m\u001b[39m\"\u001b[39m: options\u001b[39m.\u001b[39mbeam_size,\n\u001b[0;32m    601\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m: options\u001b[39m.\u001b[39mpatience,\n\u001b[0;32m    602\u001b[0m     }\n\u001b[1;32m--> 604\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    605\u001b[0m     encoder_output,\n\u001b[0;32m    606\u001b[0m     [prompt],\n\u001b[0;32m    607\u001b[0m     length_penalty\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39mlength_penalty,\n\u001b[0;32m    608\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m    609\u001b[0m     return_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    610\u001b[0m     return_no_speech_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m     suppress_blank\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39msuppress_blank,\n\u001b[0;32m    612\u001b[0m     suppress_tokens\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39msuppress_tokens,\n\u001b[0;32m    613\u001b[0m     max_initial_timestamp_index\u001b[39m=\u001b[39mmax_initial_timestamp_index,\n\u001b[0;32m    614\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    615\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    617\u001b[0m tokens \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39msequences_ids[\u001b[39m0\u001b[39m]\n\u001b[0;32m    619\u001b[0m \u001b[39m# Recover the average log prob from the returned score.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"large-v2\"\n",
    "\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "segments, info = model.transcribe(\"audio.webm\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.6fs -> %.6fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n",
      "100%|██████████| 83.0955/83.0955 [00:43<00:00,  1.91 audio seconds/s]\n"
     ]
    }
   ],
   "source": [
    "import faster_whisper\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = faster_whisper.WhisperModel(\"large-v2\", device=\"cuda\")\n",
    "\n",
    "def convert_to_hms(seconds: float) -> str:\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = math.floor((seconds % 1) * 1000)\n",
    "    output = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
    "    return output\n",
    "\n",
    "def convert_seg(segment: faster_whisper.transcribe.Segment) -> str:\n",
    "    return f\"{convert_to_hms(segment.start)} --> {convert_to_hms(segment.end)}\\n{segment.text.lstrip()}\\n\\n\"\n",
    "\n",
    "segments, info = model.transcribe(\"dvd.mp3\")\n",
    "\n",
    "full_txt = []\n",
    "timestamps = 0.0  # for progress bar\n",
    "with tqdm(total=info.duration, unit=\" audio seconds\") as pbar:\n",
    "    for i, segment in enumerate(segments, start=1):\n",
    "        full_txt.append(f\"{i}\\n{convert_seg(segment)}\")\n",
    "        pbar.update(segment.end - timestamps)\n",
    "        timestamps = segment.end\n",
    "    if timestamps < info.duration: # silence at the end of the audio\n",
    "        pbar.update(info.duration - timestamps)\n",
    "\n",
    "with open(\"file.srt\", mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "    f.writelines(full_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39m# 各セグメントからテキストを抽出\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m segment \u001b[39min\u001b[39;00m segments:\n\u001b[0;32m     14\u001b[0m     text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39msegment\u001b[39m.\u001b[39mtext \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# 関数: 文末までの文章を結合\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\faster_whisper\\transcribe.py:403\u001b[0m, in \u001b[0;36mWhisperModel.generate_segments\u001b[1;34m(self, features, tokenizer, options, encoder_output)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m encoder_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(segment)\n\u001b[0;32m    398\u001b[0m (\n\u001b[0;32m    399\u001b[0m     result,\n\u001b[0;32m    400\u001b[0m     avg_logprob,\n\u001b[0;32m    401\u001b[0m     temperature,\n\u001b[0;32m    402\u001b[0m     compression_ratio,\n\u001b[1;32m--> 403\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_with_fallback(encoder_output, prompt, tokenizer, options)\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m options\u001b[39m.\u001b[39mno_speech_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     \u001b[39m# no voice activity check\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     should_skip \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mno_speech_prob \u001b[39m>\u001b[39m options\u001b[39m.\u001b[39mno_speech_threshold\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\faster_whisper\\transcribe.py:604\u001b[0m, in \u001b[0;36mWhisperModel.generate_with_fallback\u001b[1;34m(self, encoder_output, prompt, tokenizer, options)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    600\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbeam_size\u001b[39m\u001b[39m\"\u001b[39m: options\u001b[39m.\u001b[39mbeam_size,\n\u001b[0;32m    601\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m: options\u001b[39m.\u001b[39mpatience,\n\u001b[0;32m    602\u001b[0m     }\n\u001b[1;32m--> 604\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    605\u001b[0m     encoder_output,\n\u001b[0;32m    606\u001b[0m     [prompt],\n\u001b[0;32m    607\u001b[0m     length_penalty\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39mlength_penalty,\n\u001b[0;32m    608\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m    609\u001b[0m     return_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    610\u001b[0m     return_no_speech_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m     suppress_blank\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39msuppress_blank,\n\u001b[0;32m    612\u001b[0m     suppress_tokens\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39msuppress_tokens,\n\u001b[0;32m    613\u001b[0m     max_initial_timestamp_index\u001b[39m=\u001b[39mmax_initial_timestamp_index,\n\u001b[0;32m    614\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    615\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    617\u001b[0m tokens \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39msequences_ids[\u001b[39m0\u001b[39m]\n\u001b[0;32m    619\u001b[0m \u001b[39m# Recover the average log prob from the returned score.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# インポートとWhisperモデルの設定\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"large-v2\"\n",
    "\n",
    "# WhisperModelインスタンスの作成\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "# 音声ファイルをテキストに変換\n",
    "segments, info = model.transcribe(\"dvd.mp3\", beam_size=5)\n",
    "\n",
    "text = ''\n",
    "# 各セグメントからテキストを抽出\n",
    "for segment in segments:\n",
    "    text +=segment.text + ('\\n')\n",
    "\n",
    "# 関数: 文末までの文章を結合\n",
    "def join_sentences_until_period(text):\n",
    "    sentences = text.strip().split('\\n')\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        current_result = []\n",
    "        while i < len(sentences) and not sentences[i].strip().endswith('.'):\n",
    "            current_result.append(sentences[i].strip())\n",
    "            i += 1\n",
    "        if i < len(sentences):\n",
    "            current_result.append(sentences[i].strip())\n",
    "            i += 1\n",
    "        result.append(' '.join(current_result))\n",
    "    return result\n",
    "\n",
    "# 文末までの文章を結合\n",
    "output = join_sentences_until_period(text)\n",
    "\n",
    "# 翻訳モデルのインポートとインスタンス化\n",
    "from transformers import pipeline\n",
    "fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
    "\n",
    "# 各行について翻訳を実行し、結果を出力\n",
    "for line in output:\n",
    "    # print(line)\n",
    "    print(fugu_translator(line)[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1467.919125/1467.919125 [02:19<00:00, 10.53 audio seconds/s]       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m sentences \u001b[39m=\u001b[39m split_text_into_sentences(subtitle_text)\n\u001b[0;32m     67\u001b[0m \u001b[39m# 文を日本語に翻訳\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m translated_sentences \u001b[39m=\u001b[39m translate_sentences_to_japanese(sentences)\n\u001b[0;32m     70\u001b[0m \u001b[39m# 翻訳された文を結合\u001b[39;00m\n\u001b[0;32m     71\u001b[0m translated_subtitle \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(translated_sentences)\n",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m, in \u001b[0;36mtranslate_sentences_to_japanese\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate_sentences_to_japanese\u001b[39m(sentences):\n\u001b[0;32m     60\u001b[0m     fugu_translator \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstaka/fugumt-en-ja\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     translations \u001b[39m=\u001b[39m [fugu_translator(sentence)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslation_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m translations\n",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate_sentences_to_japanese\u001b[39m(sentences):\n\u001b[0;32m     60\u001b[0m     fugu_translator \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstaka/fugumt-en-ja\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     translations \u001b[39m=\u001b[39m [fugu_translator(sentence)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslation_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m translations\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:367\u001b[0m, in \u001b[0;36mTranslationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    338\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m    Translate the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m          token ids of the translation.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1122\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1123\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         )\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1135\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1136\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1137\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1138\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1034\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1035\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1036\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1037\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\generation\\utils.py:1675\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1669\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1670\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1671\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1672\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1673\u001b[0m     )\n\u001b[0;32m   1674\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1675\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1676\u001b[0m         input_ids,\n\u001b[0;32m   1677\u001b[0m         beam_scorer,\n\u001b[0;32m   1678\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1679\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1680\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1681\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1682\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1683\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1684\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1685\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1686\u001b[0m     )\n\u001b[0;32m   1688\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1689\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\transformers\\generation\\utils.py:3014\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3010\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3012\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3014\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3015\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3016\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3017\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3018\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3019\u001b[0m )\n\u001b[0;32m   3021\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3022\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 必要なライブラリをインポート\n",
    "# import faster_whisper\n",
    "# import math\n",
    "# from tqdm import tqdm\n",
    "# from faster_whisper.transcribe import Segment\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Whisperモデルを初期化\n",
    "# model = faster_whisper.WhisperModel(\"large-v2\", device=\"cuda\")\n",
    "\n",
    "# # 音声ファイルをテキストに変換する関数\n",
    "# def transcribe_audio(audio_file):\n",
    "#     segments, info = model.transcribe(audio_file)\n",
    "#     return segments, info\n",
    "\n",
    "# # セグメントをSRT形式の文字列に変換する関数\n",
    "# def convert_to_srt(segment: Segment, index: int) -> str:\n",
    "#     return f\"{index}\\n{convert_to_hms(segment.start)} --> {convert_to_hms(segment.end)}\\n{segment.text.lstrip()}\\n\\n\"\n",
    "\n",
    "# # タイムスタンプをHH:MM:SS,mmm形式に変換する関数\n",
    "# def convert_to_hms(seconds: float) -> str:\n",
    "#     hours, remainder = divmod(seconds, 3600)\n",
    "#     minutes, seconds = divmod(remainder, 60)\n",
    "#     milliseconds = math.floor((seconds % 1) * 1000)\n",
    "#     output = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
    "#     return output\n",
    "\n",
    "# # 日本語への翻訳を行う関数\n",
    "# def translate_to_japanese(text):\n",
    "#     fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
    "#     return fugu_translator(text)[0]['translation_text']\n",
    "\n",
    "# # 音声ファイルをテキストに変換\n",
    "# segments, info = transcribe_audio(\"audio.webm\")\n",
    "\n",
    "# # SRT形式の字幕を作成\n",
    "# srt_subtitle = []\n",
    "# timestamps = 0.0\n",
    "\n",
    "# with tqdm(total=info.duration, unit=\" audio seconds\") as pbar:\n",
    "#     for i, segment in enumerate(segments, start=1):\n",
    "#         srt_subtitle.append(convert_to_srt(segment, i))\n",
    "#         pbar.update(segment.end - timestamps)\n",
    "#         timestamps = segment.end\n",
    "#     if timestamps < info.duration:  # オーディオの最後に無音がある場合\n",
    "#         pbar.update(info.duration - timestamps)\n",
    "\n",
    "# # 字幕テキストを結合\n",
    "# subtitle_text = \"\\n\".join(srt_subtitle)\n",
    "\n",
    "# # # 字幕を日本語に翻訳\n",
    "# # translated_subtitle = translate_to_japanese(subtitle_text)\n",
    "# # テキストを文ごとに分割する関数\n",
    "# def split_text_into_sentences(text):\n",
    "#     sentences = text.split('\\n')\n",
    "#     return sentences\n",
    "\n",
    "# # 日本語への翻訳を行う関数\n",
    "# def translate_sentences_to_japanese(sentences):\n",
    "#     fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
    "#     translations = [fugu_translator(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "#     return translations\n",
    "\n",
    "# # テキストを文ごとに分割\n",
    "# print(\"分割中\")\n",
    "# sentences = split_text_into_sentences(subtitle_text)\n",
    "\n",
    "\n",
    "# # 文を日本語に翻訳\n",
    "# print(\"翻訳中\")\n",
    "# translated_sentences = translate_sentences_to_japanese(sentences)\n",
    "\n",
    "# # 翻訳された文を結合\n",
    "# print(\"統合中\")\n",
    "# translated_subtitle = '\\n'.join(translated_sentences)\n",
    "\n",
    "\n",
    "# # 翻訳された字幕をファイルに保存\n",
    "# print(\"保存中\")\n",
    "# with open(\"translated_subtitle.srt\", mode=\"w\", encoding=\"UTF-8\") as f:\n",
    "#     f.write(translated_subtitle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_srt_time(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    seconds = int(seconds)\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{milliseconds:03d}\"\n",
    "\n",
    "def convert_txt_to_srt(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as txt_file:\n",
    "        lines = txt_file.readlines()\n",
    "\n",
    "    srt_lines = []\n",
    "    count = 1\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('] ')\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "\n",
    "        time_part = parts[0][1:-1]  # Remove '[' and ']'\n",
    "\n",
    "        # Check if 's' is present and remove it\n",
    "        if 's' in time_part:\n",
    "            time_part = time_part.replace('s', '')\n",
    "\n",
    "        start_time, end_time = time_part.split(' -> ')\n",
    "\n",
    "        # Convert seconds to SRT time format\n",
    "        start_time = seconds_to_srt_time(float(start_time))\n",
    "        end_time = seconds_to_srt_time(float(end_time))\n",
    "\n",
    "        srt_lines.append(f\"{count}\\n{start_time} --> {end_time}\\n{parts[1]}\\n\\n\")\n",
    "        count += 1\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as srt_file:\n",
    "        srt_file.writelines(srt_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_txt_file = \"input.txt\"\n",
    "    output_srt_file = \"output5.srt\"\n",
    "    convert_txt_to_srt(input_txt_file, output_srt_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
